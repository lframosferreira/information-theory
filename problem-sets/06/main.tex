\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\bibliographystyle{alpha}

\title{Information Theory \\ \large Problem Set 06 - Dependent Random Variables}
\author{Lu√≠s Felipe Ramos Ferreira}
\date{\href{mailto:lframos\_ferreira@outlook.com}{\texttt{lframos\_ferreira@outlook.com}}
}

\begin{document}

\maketitle

\begin{enumerate}
	\item \begin{enumerate}
		      \item \(H(X, Y)\) is the joint entropy of \(X\) and \(Y\). It means how much information, on average, each of the joint outcomes carries. ON onther words, is the expected value of information of the joint outcomes from ensembles \(X\) and \(Y\).
		            \[H(X, Y) = \sum_{xy \in \mathcal{A}_x\mathcal{A}_y} P(x, y) log \frac{1}{P(x, y)}\]
		      \item \(H(X | Y)\) is the conditional entropy of \(X\) given \(Y\). It represents the average information infomration content of \(X\) given each \(y \in \mathcal{A}_y\).
		            \[H(X | Y) = \sum_{xy \in \mathcal{A}_x\mathcal{A}_y} P(x, y) log \frac{1}{P(x | y)}\]
		      \item \(I(X, Y)\) is the mutual information between \(X\) and \(Y\). It is the average reduction of uncertainty/ gain of information about \(X\) when learning the values of \(Y\), or vice-versa.
		            \[I(X; Y) = H(X) - H(X | Y) \]
		      \item \(I(X: Y | Z)\) is the conditional mutual information between \(X\) and \(Y\) given \(Z\). It is the mututal information between \(X\) and \(Y\) given that the ensemble \(Z\) is known.
		            \[I(X; Y | Z) = H(X | Z) - H(X | Y, Z)\]
	      \end{enumerate}

	\item \begin{enumerate}
		      \item a
		      \item b
		      \item c
	      \end{enumerate}

	\item 
        \[H(X, Y) = \sum_{xy \in \mathcal{A}_x \mathcal{A}_y} P(x, y) log \frac{1}{P(x, y)} = \]
	\item 4
	\item 5
	\item 6
	\item 7
	\item 8
\end{enumerate}
\end{document}
