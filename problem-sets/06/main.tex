\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\bibliographystyle{alpha}

\title{Information Theory \\ \large Problem Set 06 - Dependent Random Variables}
\author{Lu√≠s Felipe Ramos Ferreira}
\date{\href{mailto:lframos\_ferreira@outlook.com}{\texttt{lframos\_ferreira@outlook.com}}
}

\begin{document}

\maketitle

\begin{enumerate}
	\item \begin{enumerate}
		      \item \(H(X, Y)\) is the joint entropy of \(X\) and \(Y\). It means how much information, on average, each of the joint outcomes carries. ON onther words, is the expected value of information of the joint outcomes from ensembles \(X\) and \(Y\).
		            \[H(X, Y) = \sum_{xy \in \mathcal{A}_x\mathcal{A}_y} P(x, y) log \frac{1}{P(x, y)}\]
		      \item \(H(X | Y)\) is the conditional entropy of \(X\) given \(Y\). It represents the average information infomration content of \(X\) given each \(y \in \mathcal{A}_y\).
		            \[H(X | Y) = \sum_{xy \in \mathcal{A}_x\mathcal{A}_y} P(x, y) log \frac{1}{P(x | y)}\]
		      \item \(I(X, Y)\) is the mutual information between \(X\) and \(Y\).
		      \item d
	      \end{enumerate}

	\item \begin{enumerate}
		      \item a
		      \item b
		      \item c
	      \end{enumerate}

	\item 3
	\item 4
	\item 5
	\item 6
	\item 7
	\item 8
\end{enumerate}
\end{document}
