\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[square, numbers]{natbib}
\bibliographystyle{unsrtnat}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{xcolor}
\bibliographystyle{alpha}

\title{Information Theory \\ \large Problem Set 10 - Advanced Information Measures}
\author{Lu√≠s Felipe Ramos Ferreira}
\date{\href{mailto:lframos\_ferreira@outlook.com}{\texttt{lframos\_ferreira@outlook.com}}
}

\begin{document}

\maketitle

\begin{enumerate}
	\item \begin{enumerate}
		      \item Given a set of secret values, we can define the set of all posible probability distributions over this set of secret values. A state of knowledge of a agent about
		            the secret (or state of the world) can be defined as one of such probability distributions that the set of posible secrets can have.

		      \item A information measure is a mapping from a state of knowledge to a real number, i.e, it's a function \(f: \mathcal{X} \mathbb{D} \rightarrow \mathbb{R}\), where \(\mathcal{X} \mathbb{D}\)
		            is a set of probability distributions about a set of secrets \(\mathcal{X}\), \(f\) is the function itself and \(\mathbb{R}\) is a real number that represents how much knowledge the agent had about the secret
		            before knowing it's value.
		      \item The mathematical definiton of information measure is a function that maps every posible input of possible states of knowledge to a real number that represents the value of that knowledge. The operational significance
		            of information measure it's the meaning of the real number returned by the function mentioned, i.e., what that value means and how can we interpretate it.
		      \item \begin{enumerate}
			            \item Shannons's entropy is formally defined as the function \[H(\pi) = -\sum_{x \in \mathbb{X}} \pi_x log_2(\pi_x)\], where \(\pi\) is a probability distribution over the set of secrets \(\mathbb{X}\)
			                  Shannon's entropy operational significance is the expected number of questions an agent would ask to discover the secret when using an optimal binary search.
			            \item Bayes vulnerability is formally defined as the function \[V(\pi) = \max_{x \in \mathbb{X}} \pi_x\]. Bayes vulnerability operational
			                  significance is the probability that an agent can guess the secret with only one try.
			            \item Guessing entropy is formally defined as the function \[G(\pi) = \sum_k \pi_k k\], where \(k\) defines
			                  a non increasing ordering of the probability distribution \(\pi_i\). Gyessing entropy operational significance is the expected
			                  number of guesses an agent would need to try in order to discover the secret in a optimal linear search.
		            \end{enumerate}
		      \item We can define \(\mathbb{X}\) the set of secrets and \(mathbb{W}\) the set of posible actions the adversary
		            can take in order to discover the secrets we want to hide. The function \[g: \mathbb{W} \times \mathbb{X} \rightarrow \mathbb{R}\]
		            can be seen as a gain function that represents the gain of the adversary when he takes action \(w\) and the secret value is \(x\).
		            In this context, the g-vulnerability of a distribution \(\pi\) is a information measure formally defined as \[V_g[\pi] = \max_{w \in \mathbb{W} \sum_{x \in \mathbb{X}} \pi_x g(w, x)}\]
		            It represents the expected gain of a rational adversary taking a best action.
		      \item When the adversary observes the output of a channel, it's state of knowledge about the world changes, i.e.,
		            it's prior probability distribution that represented his knowledge about the secret becomes a set of conditional probabilites given by the observation
		            of the output of the channel.
		      \item With a prior probability distribution \(pi\) as the prior state fo the knowledge about the secret, \(g\) as a gain function
		            and \(C\) a channel matrix from \(\mathcal{X}\) to \(\mathcal{Y}\), the posterior g-vulnerability can be formally
		            defined as a function that maps a posterior state of knowledge to a real number and is defined by:

		            \[V_g[\pi \rangle C] = \sum_{y \in \mathcal{Y} \text{ and } p(y) \neq 0} p(y) V_g(p(X \mid y)\]

		            It is interpreted as the expected optimal gain of a rational adversary given all possible outputs of the channel.
		      \item Leakage, in this conext, represents the amount of information the adversary gains about the secret by the output of the chanenl.
		            Given a prior state of knowledge probaility distribution \(\pi\), a gain function \(g\) and a channel \(C\),
		            the leakage of information of a channel is formally defined by:
		            \begin{itemize}
			            \item Multiplicative g-leakage: the ratio of increase in the information the adversary has
			                  \[\mathbb{L}_{g}^{x}(\pi, C) = \frac{V_g[\pi \rangle C]}{V_g(\pi)}\]
			            \item Additive g-leakage: the absolute increase in the information the adversary has
			                  \[\mathbb{L}_{g}^{+}(\pi, C) = V_g[\pi \rangle C] - V_g(\pi)\]
		            \end{itemize}
	      \end{enumerate}
	\item
	      \begin{enumerate}
		      \item We call \(\mathbb{X}\) the set of posible secrets. Since each voter can only make a binary choice, candidate
		            A or B, there are \(2^k\) elements in \(\mathbb{X}\). Since the prior state of knowledge is the uniform distribution \(\pi_u\),
		            we have that the Bayes vulnerability of the system is:
		            \[V(\pi_u) = \max_{x \in \mathbb{X}} \pi_u(x) = 2^{-k}\]
		      \item Since \(k = 3\), i.e, there are \(3\) voters, the set of posible secrets/votes is \(\mathbb{X} = \{AAA, AAB, ABB, BBB, ABA, BAB, BAA, BBA\}\), where each triple \(v_1v_2v_3\) represents for which candidate each voter voted. The output of the channel can be represented
		            by the set \(\mathbb{Y} = \{(0, 3), (1, 2), (2, 1), (3, 0)\}\), where each tuple \((a, b)\) represents the number of votes candidates A and B got. The channel \(C\) can be represented by the following table:
		            \begin{table}[H]
			            \centering
			            \begin{tabular}{|c|c|c|c|c|}
				            \hline
				            C   & (0, 3) & (1, 2) & (2, 1) & (3, 0) \\ \hline
				            AAA & 0      & 0      & 0      & 1      \\ \hline
				            AAB & 0      & 0      & 1      & 0      \\ \hline
				            ABA & 0      & 0      & 1      & 0      \\ \hline
				            ABB & 0      & 1      & 0      & 0      \\ \hline
				            BAA & 0      & 0      & 1      & 0      \\ \hline
				            BAB & 0      & 1      & 0      & 0      \\ \hline
				            BBA & 0      & 1      & 0      & 0      \\ \hline
				            BBB & 1      & 0      & 0      & 0      \\ \hline
			            \end{tabular}
			            \caption{\(C\)}
		            \end{table}
		      \item Since \(\pi_u\) is a prior uniform distribution and \(k = 3\), the prior Bayes vulnerability is:
		            \[V[\pi_u] = \max_x \pi_u(x) = \frac{1}{8}\]
		            Using the values for \(\pi_u\) and \(C\), we can construct the joint distribution \(\mathbb{J}\) below:
		            \begin{table}[H]
			            \centering
			            \begin{tabular}{|c|c|c|c|c|}
				            \hline
				            J(x, y) & (0, 3)        & (1, 2)        & (2, 1)        & (3, 0)        \\ \hline
				            AAA     & 0             & 0             & 0             & $\frac{1}{8}$ \\ \hline
				            AAB     & 0             & 0             & $\frac{1}{8}$ & 0             \\ \hline
				            ABA     & 0             & 0             & $\frac{1}{8}$ & 0             \\ \hline
				            ABB     & 0             & $\frac{1}{8}$ & 0             & 0             \\ \hline
				            BAA     & 0             & 0             & $\frac{1}{8}$ & 0             \\ \hline
				            BAB     & 0             & $\frac{1}{8}$ & 0             & 0             \\ \hline
				            BBA     & 0             & $\frac{1}{8}$ & 0             & 0             \\ \hline
				            BBB     & $\frac{1}{8}$ & 0             & 0             & 0             \\ \hline
			            \end{tabular}
			            \caption{\(J(x, y)\)}
		            \end{table}
		            With these values, we can then compute the set of posterior distributions of \(X\) given the value
		            of the output \(Y\), and also the values of the posterior probabilities on \(Y\).

		            \begin{table}[H]
			            \centering
			            \begin{tabular}{|c|c|c|c|c|}
				            \hline
				            p(x | y) & (0, 3) & (1, 2)        & (2, 1)        & (3, 0) \\ \hline
				            AAA      & 0      & 0             & 0             & 1      \\ \hline
				            AAB      & 0      & 0             & $\frac{1}{3}$ & 0      \\ \hline
				            ABA      & 0      & 0             & $\frac{1}{3}$ & 0      \\ \hline
				            ABB      & 0      & $\frac{1}{3}$ & 0             & 0      \\ \hline
				            BAA      & 0      & 0             & $\frac{1}{3}$ & 0      \\ \hline
				            BAB      & 0      & $\frac{1}{3}$ & 0             & 0      \\ \hline
				            BBA      & 0      & $\frac{1}{3}$ & 0             & 0      \\ \hline
				            BBB      & 1      & 0             & 0             & 0      \\ \hline
			            \end{tabular}
			            \caption{Posterior hyper distribution}
		            \end{table}


		            \begin{table}[H]
			            \centering
			            \begin{tabular}{|c|c|}
				            \hline
				            y      & p(y)          \\ \hline
				            (0, 3) & $\frac{1}{8}$ \\ \hline
				            (1, 2) & $\frac{3}{8}$ \\ \hline
				            (2, 1) & $\frac{3}{8}$ \\ \hline
				            (3, 0) & $\frac{1}{8}$ \\ \hline
			            \end{tabular}
			            \caption{Posterior probabilites on Y}
		            \end{table}
		            With this values calculated, we can compute the posterior Bayes vulnerability:

		            \[V[\pi_u, C] = \sum_y p(y) max_x p(x \mid y) = \frac{1}{8} + \frac{3}{8} \frac{1}{3} + \frac{3}{8} + \frac{1}{8} = \frac{1}{2}\]

		            And the multiplicative Bayes leakage, which can be calculated with:

		            \[\frac{V[\pi_u, C]}{V[\pi_u]} = \frac{1}{2} \frac{8}{1} = 4\]
	      \end{enumerate}
\end{enumerate}

\bibliography{sample}
\nocite{*}

\end{document}
